{"cells":[{"cell_type":"code","source":["#### --- The data which we have acquired is loans data from Lending Club ---- ####\n####   The main objective of the project is to predict whether a loan will be Charged off or Fully paid #####\n####   The initial data had close to a Million rows and 72 columns #### \n#### We also plan to understand factors and key points in clientâ€™s record which makes them a good client ####\n\n### Importing pyspark.sql functions ### \nfrom pyspark.sql import functions as fn \n\n#Reading the file \ndata = sqlContext.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/loan.csv')  \n\n#selecting a column from the data set to check the data \nid1 = data.select('id')\ndata.select('id').show(5)\n\n#Dropping the columns which are not required for the classification and storing it in new variable data_clean\ndata_clean = data.drop(\"member_id\",\"issue_d\",\"home_ownership\",\"emp_title\",\"url\",\"desc\",\"purpose\",\"title\",\"zip_code\",\"addr_state\",\"last_pymnt_d\",\"sub_grade\")\n\n##Removing the loan whose status is Issued \ndata_clean = data_clean.where(\"loan_status != 'Issued'\")\n\n## Removing all the loans for wich status is current \ndata_clean = data_clean.where(\"loan_status != 'Current'\")\n\n## Removing the loans for  31-120 days late payment \ndata_clean = data_clean.where(\"loan_status != 'Late (31-120 days)'\")\n\n## Removing the loans for  16-120 days late payment \ndata_clean = data_clean.where(\"loan_status != 'Late (16-30 days)'\") \n\n#Only Takking the loans which are of the status Charged Off or Fully Paid \ndata_clean = data_clean.where(\"loan_status IN ('Charged Off','Fully Paid')\") \n\n##Check the number of remaining rows \ndata_clean.count()\n### Dividing the data to avoid \ndisplay(data_clean)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["### Checking the columns which are \"NULL\" and based on the number of columns which have more than 20% NULL values, the columns will be removed in the codes below\n\nfrom pyspark.sql.functions import isnan, when, count, col\nncn1=data_clean.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in data_clean.columns]).show()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["## Dropping the columns based on the Null Values it contains. \n## The impact of these columns on the final result that we are predicting is next to negligible thus removing these columns  \n\ndata_clean= data_clean.drop(\"last_fico_range_high\",\"last_fico_range_low\",\"earliest_cr_line\",\"last_pymnt_amnt\",\"mths_since_last_major_derog\",\"mths_since_last_record\",\"next_pymnt_d\",\"policy_code\",\"pymnt_plan\",\"recoveries\",\"revol_bal\",\"revol_util\",\"title\",\"total_acc\",\"total_pymnt_imv\",\"total_rec_int\",\"total_rec_prncp\",\"open_rev_12m\",\"open_rev_24m\",\"open_il_12m\",\"open_il_12m\",\"mths_since_rcnt_il\",\"il_util\",\"inq_fi\",\"total_cu_tl\",\"inq_last_12m\",\"tot_coll_amt\",\"tot_cur_bal\",\"member_id\",\"issue_d\",\"home_ownership\",\"emp_title\",\"url\",\"desc\",\"purpose\",\"title\",\"zip_code\",\"addr_state\",\"last_pymnt_d\",\"sub_grade\",\"total_rev_hi_lim\",\"all_util\",\"max_bal_bc\",\"open_rv_24m\",\"open_rv_12m\",\"total_bal_il\",\"open_il_24m\",\"open_acc_6m\",\"verification_status_joint\",\"dti_joint\",\"annual_inc_joint\",\"mths_since_last_delinq\",\"open_il_6m\")\n#display(data_clean)\n\n#display(data_clean)\ndf123 = data_clean.where(\"loan_status IN ('Fully Paid')\")\nfd = data_clean.where(\"loan_status IN ('Charged Off')\")\n(df123take, dont) = df123.randomSplit([0.3, 0.7])\nfrom functools import reduce  # For Python 3.x\nfrom pyspark.sql import DataFrame\n\ndef unionAll(*dfs):\n    return reduce(DataFrame.unionAll, dfs)\n\ndata_clean = unionAll(df123take, fd)\n#data_clean.count()\ndf123take.count()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["## Checking the distinct application types for the data \n\ndisplay(data_clean.select(data_clean[\"application_type\"]).distinct())\ndata_clean.count()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["## Since there were junk values in the applicaion type we are only taking the  types Joint and Individual columns \n\ndata_clean = data_clean.where(\"application_type IN ('INDIVIDUAL','JOINT')\")\n\n### Counting the final number of rows \ndata_clean.count()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["#### This block of code deals with data cleaning process \n#### In the block of code the length of the employment for any employee is converted into Integer values from strings\n\n#### The assumptions we have made in this sections are:    1. n/a is considered as 0  2. 10+ years  is considered as 11  3. <1 year is considered as 0.5 \n\nfrom pyspark.sql.functions import *\n\n#replacing the employee length value with zero \ndata_clean= data_clean.withColumn('emp_length', regexp_replace('emp_length', 'n/a', '0'))\ndata_clean.where(\"emp_length IN ('0')\").count()\n\n#Replacing other values of employee length \n\ndata_clean= data_clean.withColumn('emp_length', regexp_replace('emp_length', '9 years', '9'))\ndata_clean.where(\"emp_length IN ('9')\").count()\n\ndata_clean= data_clean.withColumn('emp_length', regexp_replace('emp_length', '8 years', '8'))\ndata_clean.where(\"emp_length IN ('8')\").count()\n\ndata_clean= data_clean.withColumn('emp_length', regexp_replace('emp_length', '7 years', '7'))\ndata_clean.where(\"emp_length IN ('7')\").count()\n\ndata_clean= data_clean.withColumn('emp_length', regexp_replace('emp_length', '6 years', '6'))\ndata_clean.where(\"emp_length IN ('6')\").count()\n\ndata_clean= data_clean.withColumn('emp_length', regexp_replace('emp_length', '5 years', '5'))\ndata_clean.where(\"emp_length IN ('5')\").count()\n\ndata_clean= data_clean.withColumn('emp_length', regexp_replace('emp_length', '4 years', '4'))\ndata_clean.where(\"emp_length IN ('4')\").count()\n\ndata_clean= data_clean.withColumn('emp_length', regexp_replace('emp_length', '3 years', '3'))\ndata_clean.where(\"emp_length IN ('3')\").count()\n\ndata_clean= data_clean.withColumn('emp_length', regexp_replace('emp_length', '2 years', '2'))\ndata_clean.where(\"emp_length IN ('2')\").count()\n\ndata_clean= data_clean.withColumn('emp_length', regexp_replace('emp_length', '1 year', '1'))\ndata_clean.where(\"emp_length IN ('1')\").count()\n\n## As seen below the column was not replaced properly thus need to try different approach \ndata_clean = data_clean.withColumn('emp_length', regexp_replace('emp_length', \"< 1 year\", \"0.5\"))\ndata_clean.where(\"emp_length IN ('0.5')\").count()\n\ndata_clean = data_clean.withColumn('emp_length', regexp_replace('emp_length', \"< 1\", \"0.5\"))\ndata_clean.where(\"emp_length IN ('0.5')\").count()\n\n## As seen below the column was not replaced properly thus need to try different approach \n#from pyspark.sql import SQLContext\ndata_clean = data_clean.withColumn('emp_length', regexp_replace('emp_length', '10+ years', '11'))\ndata_clean.where(\"emp_length IN ('11')\").count()\n\ndata_clean = data_clean.withColumn('emp_length', regexp_replace('emp_length', '10\\\\+ years', '11'))\ndata_clean.where(\"emp_length IN ('11')\").count()\n\ndata_clean.count()\ndisplay(data_clean)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["## Creating dummy variables for the categorical type of data \n\n## verification_status is of Multiple Types Verfied, Source Verified, Not Verified. Thus to consider these values we have taken the dummy variables since to understand the impact of these variables on the final result\n\n## Similarly we are also converting Loan_Status as which is our prediction column, Application type, term, verification status. \n\nimport pyspark.sql.functions as F\nverification = data_clean.select(\"verification_status\").distinct().rdd.flatMap(lambda x: x).collect()\nstatus = data_clean.select(\"loan_status\").distinct().rdd.flatMap(lambda x: x).collect()\napplication = data_clean.select(\"application_type\").distinct().rdd.flatMap(lambda x: x).collect()\nterm = data_clean.select(\"term\").distinct().rdd.flatMap(lambda x: x).collect()\nveri_expr = [F.when(F.col(\"Verification_status\") == vs, 1).otherwise(0).alias(\"V_Status_\" + vs) for vs in verification]\nLoan_expr = [F.when(F.col(\"loan_status\") == ls, 1).otherwise(0).alias(\"Loan_Stat_\" + ls) for ls in status]\napp_expr = [F.when(F.col(\"application_type\") == at, 1).otherwise(0).alias(\"App_Type_\" + at) for at in application]\nterm_expr = [F.when(F.col(\"term\") == t, 1).otherwise(0).alias(\"term_Type_\" + t) for t in term]\ndummy = data_clean.select(\"Verification_status\",\"application_type\",\"term\", \"id\",\"loan_status\",*veri_expr + app_expr  +term_expr +Loan_expr)\ndisplay(dummy)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["##Joining the dummy variable data_clean dataframe with dummy data frame \ndf = data_clean.join(dummy, (data_clean.id == dummy.id), \"inner\" )\n\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["#term 0 is 36 and 1 is 60 months\n#App_type 0 is INDIVIDUAL and 1 is JOINT\n#Loan_stat_ 0 for fully paid and 1 for Charged off\n#V_Status verified is 00, Source Verified is 10 and Not Verified is 01\n\n## Dropping the columns which are redundant in the nature  \n\ndf = df.drop(\"id\",\"term_Type_36 months\",\"App_Type_INDIVIDUAL\",\"Loan_Stat_Fully Paid\",\"V_Status_Verified\",\"Verification_status\",\"loan_status\",\"application_type\",\"term\",\"grade\",\"initial_list_status\",\"last_credit_pull_d\")\n\ndf.count()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["##Casting all the columns to float \ndf.select(*(col(c).cast(\"float\").alias(c) for c in df.columns))\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["#Checking the NULL Values if there is any in the filtered dataset  \n\nncn2=df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["## Dropping the rows which had 'N/A' since the number of rows were only 56\ndfnan = df.na.drop(subset=[\"collections_12_mths_ex_med\"])\n\n#from pyspark.sql.functions import isnan, when, count, col\nncn3=dfnan.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in dfnan.columns]).show()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["### Changing the dataframe to a dense vector dataframe in order to create the input data frames for the models \n\ndef transData(df):\n    return df.rdd.map(lambda r: [r[-1], Vectors.dense(r[:-1])]).\\\n           toDF(['label','features1'])\n\nfrom pyspark.sql import Row\nfrom pyspark.ml.linalg import Vectors\n\ndata= transData(dfnan)\n#data.show()\n\ndisplay(data)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["### Changing the dataframe to a dense vector dataframe in order to create the input data frames for running the PCA  \n\ndef transData(df):\n    return df.rdd.map(lambda r: [r[-1], Vectors.dense(r[:])]).\\\n           toDF(['label','features1'])\n\nfrom pyspark.sql import Row\nfrom pyspark.ml.linalg import Vectors\n\ndata1= transData(dfnan)\n#data.show()\n\ndisplay(data1)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["## We are scaling the data for running coorelation matrix\n## Using Mix Max Scalar we have scaled the data before running the models \n\nfrom pyspark.ml.feature import MinMaxScaler\nscaler = MinMaxScaler(inputCol=\"features1\", outputCol=\"features\")\n# Compute summary statistics and generate MinMaxScalerModel\n\n\nscalerModel1 = scaler.fit(data1)\n\n# rescale each feature to range [min, max].\n\nscaledData1 = scalerModel1.transform(data1)\n\nprint(\"Features scaled to range: [%f, %f]\" % (scaler.getMin(), scaler.getMax()))\n\nscaledData1.select(\"features1\", \"features\").show()\n\ndisplay(scaledData1)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["### Correlation matrix \n\nrdd = df2.rdd\nfeatures = rdd.map(lambda row: row[1:])\n\nfrom pyspark.mllib.stat import Statistics\n\ncorr_mat=Statistics.corr(features, method=\"spearman\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["## Displaying the matrix \ntype(corr_mat)\ncorr_mat"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["## Displaying the matrix for correlation matrix \nimport pandas as pd\npd.set_option('display.max_columns', 26)\n\ncol_names = [\"loan_amnt\",\"funded_amnt\"\n,\"funded_amnt_inv\"\n,\"int_rate\"\n,\"installment\"\n,\"emp_length\"\n,\"annual_inc\"\n,\"dti\"\n,\"delinq_2yrs\"\n,\"inq_last_6mths\"\n,\"open_acc\"\n,\"pub_rec\"\n,\"out_prncp\"\n,\"out_prncp_inv\"\n,\"total_pymnt\"\n,\"total_pymnt_inv\"\n,\"total_rec_late_fee\"\n,\"collection_recovery_fee\"\n,\"collections_12_mths_ex_med\"\n,\"acc_now_delinq\"\n,\"V_Status_Source Verified\"\n,\"V_Status_Not Verified\"\n,\"App_Type_JOINT\"\n,\"term_Type_ 36 months\"\n,\"term_Type_ 60 months\"\n]\n\ncorr_df = pd.DataFrame(\n                    corr_mat, \n                    index=col_names, \n                    columns=col_names)\n\ncorr_df"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["#################################################################################################################################################\n#df1 = df1.selectExpr(\"scaledFeaures\", \"features\")\n#df1\n## We are taking into consideration 3 models which are based on the result of coorelation matrix\n\n##Model 1 data \n## First Model Consists of Interest Rate, Employee length, Verification Status\n\ndfnan_m1 = dfnan.select(\"int_rate\", \"emp_length\", \"V_Status_Not Verified\" , \"Loan_Stat_Charged Off\")\n\n##Model 2 Data\n##Seccond Consists of Interest Rate, Employee length, Verification Status, Total Rec Late Fee, Annual Inc, Debt to Income Ratio, Verfication Status, Term\ndfnan_m2 = dfnan.select(\"int_rate\", \"total_rec_late_fee\", \"loan_amnt\", \"annual_inc\", \"dti\", \"V_Status_Not Verified\", \"term_Type_ 36 months\", \"Loan_Stat_Charged Off\")\n\n##Model3 Data consists of Total Payment, Inq Last 6 Months, Verification Status, Open Account \ndfnan_m3 = dfnan.select( \"total_pymnt\", \"inq_last_6mths\" ,\"V_Status_Source Verified\",\"open_acc\", \"Loan_Stat_Charged Off\")\n\ndef transData(df):\n    return df.rdd.map(lambda r: [r[-1], Vectors.dense(r[:-1])]).\\\n           toDF(['label','features1'])\n\nfrom pyspark.sql import Row\nfrom pyspark.ml.linalg import Vectors\n\ndata_m1= transData(dfnan_m1)\ndata_m2 = transData(dfnan_m2)\ndata_m3 = transData(dfnan_m3)\n#data.show()\n\n\nfrom pyspark.ml.feature import MinMaxScaler\nscaler = MinMaxScaler(inputCol=\"features1\", outputCol=\"features\")\n\n# Compute summary statistics and generate MinMaxScalerModel\nscalerModel_m1 = scaler.fit(data_m1)\nscalerModel_m2 = scaler.fit(data_m2)\nscalerModel_m3 = scaler.fit(data_m3)\n\n# rescale each feature to range [min, max].\nscaledData_m1 = scalerModel_m1.transform(data_m1)\nscaledData_m2 = scalerModel_m2.transform(data_m2)\nscaledData_m3 = scalerModel_m3.transform(data_m3)\n\n\nprint(\"Features scaled to range: [%f, %f]\" % (scaler.getMin(), scaler.getMax()))\nscaledData_m1.select(\"features1\", \"features\").show()\ndisplay(scaledData_m1)\n\n#display(data)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["### Taking only the scaled features column forward and dropping the non required column \nm1 = scaledData_m1.drop(\"features1\")\nm2 = scaledData_m2.drop(\"features1\")\nm3 = scaledData_m3.drop(\"features1\")\ndf2 = scaledData1.drop(\"features1\")\ndisplay(df2)\n"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["#Applying Principal Component Analysis for Dimensionality Reduction\nfrom pyspark.ml.feature import PCA\n\npca = PCA(k=10, inputCol=\"features\", outputCol=\"pcaFeatures\")\nmodel = pca.fit(df1)\n\nresult = model.transform(df1).select(\"pcaFeatures\")\nresult.show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["#Randomly splitting the data into Training, Validation and Testing set for Model1\n\n# Also counting the number of number of rows in each of the dataset \n\n(training_m1, valid_m1, test_m1) = m1.randomSplit([0.7, 0.2, 0.1])\ntraining_m1.groupBy('label').agg(fn.count('*')).show()\ntest_m1.groupBy('label').agg(fn.count('*')).show()\nm1.groupBy('label').agg(fn.count('*')).show()\n\n"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["#Randomly splitting the data into Training, Validation and Testing set for Model2\n\n# Also counting the number of number of rows in each of the dataset \n\n(training_m2, valid_m2, test_m2) = m2.randomSplit([0.7, 0.2, 0.1])\ntraining_m2.groupBy('label').agg(fn.count('*')).show()\ntest_m2.groupBy('label').agg(fn.count('*')).show()\nm2.groupBy('label').agg(fn.count('*')).show()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["#Randomly splitting the data into Training, Validation and Testing set for Model3\n\n# Also counting the number of number of rows in each of the dataset \n\n(training_m3, valid_m3, test_m3) = m3.randomSplit([0.7, 0.2, 0.1])\ntraining_m3.groupBy('label').agg(fn.count('*')).show()\ntest_m3.groupBy('label').agg(fn.count('*')).show()\nm3.groupBy('label').agg(fn.count('*')).show()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["## Logistic regression \n\n## We are training the dataset for Model1 \n## The results of the training are displayed below with the coefficients and the Intercept \n\nfrom pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(regParam=0.3)\nlrModel = lr.fit(training_m1)\nprint(\"Coefficients: \\n\" + str(lrModel.coefficientMatrix))\nprint(\"Intercept: \" + str(lrModel.interceptVector))"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["#Validating the data for for model1. As we can see below, the accuracy for the model is 63.91 \n\nlr = LogisticRegression().\\\n    setLabelCol('label').\\\n    setFeaturesCol('features').\\\n    setRegParam(0.0).\\\n    setMaxIter(100).\\\n    setElasticNetParam(0.)\nLRmodelm1 = lr.fit(training_m1)\nLRmodelm1.transform(valid_m1).\\\n    select(fn.expr('float(prediction = label)').alias('correct')).\\\n    select(fn.avg('correct')).show()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["## Logistic regression \n\n## We are training the dataset for Model2\n## The results of the training are displayed below with the coefficients and the Intercept \n\nfrom pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(regParam=0.3)\nlrModel = lr.fit(training_m2)\nprint(\"Coefficients: \\n\" + str(lrModel.coefficientMatrix))\nprint(\"Intercept: \" + str(lrModel.interceptVector))"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["#Validating the data for for model1. As we can see below, the accuracy for the model is 66.58\nlr = LogisticRegression().\\\n    setLabelCol('label').\\\n    setFeaturesCol('features').\\\n    setRegParam(0.0).\\\n    setMaxIter(100).\\\n    setElasticNetParam(0.)\nLRmodelm1 = lr.fit(training_m2)\nLRmodelm1.transform(valid_m2).\\\n    select(fn.expr('float(prediction = label)').alias('correct')).\\\n    select(fn.avg('correct')).show()"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(regParam=0.3)\nlrModel = lr.fit(training_m3)\nprint(\"Coefficients: \\n\" + str(lrModel.coefficientMatrix))\nprint(\"Intercept: \" + str(lrModel.interceptVector))"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["#Validating the data for for model1. As we can see below, the accuracy for the model is 74.24\nlr = LogisticRegression().\\\n    setLabelCol('label').\\\n    setFeaturesCol('features').\\\n    setRegParam(0.0).\\\n    setMaxIter(100).\\\n    setElasticNetParam(0.)\nLRmodelm1 = lr.fit(training_m3)\nLRmodelm1.transform(valid_m3).\\\n    select(fn.expr('float(prediction = label)').alias('correct')).\\\n    select(fn.avg('correct')).show()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["## We are implementing decision Tree our Model1\n\nfrom pyspark import SparkContext, SQLContext\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(training_m1)\n# Automatically identify categorical features, and index them.\n# We specify maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer =\\\n    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(training_m1)\n\n# Train a DecisionTree model.\ndt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n\n# Chain indexers and tree in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(training_m1)\n\n# Make predictions.\npredictions = model.transform(valid_m1)\n\n# Select example rows to display.\npredictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g \" % (1.0 - accuracy))\nprint(accuracy)\ntreeModel = model.stages[2]\n# summary only\nprint(treeModel)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["#### We are implementing decision Tree our Model2\n\nfrom pyspark import SparkContext, SQLContext\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(training_m2)\n# Automatically identify categorical features, and index them.\n# We specify maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer =\\\n    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(training_m2)\n\n# Train a DecisionTree model.\ndt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n\n# Chain indexers and tree in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(training_m2)\n\n# Make predictions.\npredictions = model.transform(valid_m2)\n\n# Select example rows to display.\npredictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g \" % (1.0 - accuracy))\nprint(accuracy)\ntreeModel = model.stages[2]\n# summary only\nprint(treeModel)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["## We are implementing decision Tree our Model3\n\nfrom pyspark import SparkContext, SQLContext\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(training_m3)\n# Automatically identify categorical features, and index them.\n# We specify maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer =\\\n    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(training_m3)\n\n# Train a DecisionTree model.\ndt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n\n# Chain indexers and tree in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(training_m3)\n\n# Make predictions.\npredictions = model.transform(valid_m3)\n\n# Select example rows to display.\npredictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g \" % (1.0 - accuracy))\nprint(accuracy)\ntreeModel = model.stages[2]\n# summary only\nprint(treeModel)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["## We are implementing Random Forest for Model1\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(training_m1)\n\n# Automatically identify categorical features, and index them.\n# Set maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer =\\\n    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(training_m1)\n\n# Train a RandomForest model.\nrf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)\n\n# Convert indexed labels back to original labels.\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n                               labels=labelIndexer.labels)\n\n# Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(training_m1)\n\n# Make predictions.\npredictions = model.transform(valid_m1)\n\n# Select example rows to display.\npredictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\nprint(accuracy)\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["## We are implementing Random Forest for Model2\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(training_m2)\n\n# Automatically identify categorical features, and index them.\n# Set maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer =\\\n    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(training_m2)\n\n# Train a RandomForest model.\nrf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)\n\n# Convert indexed labels back to original labels.\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n                               labels=labelIndexer.labels)\n\n# Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(training_m2)\n\n# Make predictions.\npredictions = model.transform(valid_m2)\n\n# Select example rows to display.\npredictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\nprint(accuracy)\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["## We are implementing Random Forest for Model3\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(training_m3)\n\n# Automatically identify categorical features, and index them.\n# Set maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer =\\\n    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(training_m3)\n\n# Train a RandomForest model.\nrf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)\n\n# Convert indexed labels back to original labels.\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n                               labels=labelIndexer.labels)\n\n# Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(training_m3)\n\n# Make predictions.\npredictions = model.transform(valid_m3)\n\n# Select example rows to display.\npredictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\nprint(accuracy)\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["## We are implementing Gradiant Boosting Trees for Model1\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(training_m1)\n# Automatically identify categorical features, and index them.\n# Set maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer =\\\n    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(training_m1)\n\n# Train a GBT model.\ngbt = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", maxIter=10)\n\n# Chain indexers and GBT in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, gbt])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(training_m1)\n\n# Make predictions.\npredictions = model.transform(valid_m1)\n\n# Select example rows to display.\npredictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\ngbtModel = model.stages[2]\nprint(gbtModel)  # summary only\nprint(accuracy)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["## We are implementing Gradiant Boosting Trees for Model2\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(training_m2)\n# Automatically identify categorical features, and index them.\n# Set maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer =\\\n    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(training_m2)\n\n# Train a GBT model.\ngbt = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", maxIter=10)\n\n# Chain indexers and GBT in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, gbt])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(training_m2)\n\n# Make predictions.\npredictions = model.transform(valid_m2)\n\n# Select example rows to display.\npredictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\ngbtModel = model.stages[2]\nprint(gbtModel)  # summary only\nprint(accuracy)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["## We are implementing Gradiant Boosting Trees for Model3\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(training_m3)\n# Automatically identify categorical features, and index them.\n# Set maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer =\\\n    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(training_m3)\n\n# Train a GBT model.\ngbt = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", maxIter=10)\n\n# Chain indexers and GBT in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, gbt])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(training_m3)\n\n# Make predictions.\npredictions = model.transform(valid_m3)\n\n# Select example rows to display.\npredictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\ngbtModel = model.stages[2]\nprint(gbtModel)  # summary only\nprint(accuracy)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["## We are implementing Naive Bayes for Model1\n\nfrom pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# create the trainer and set its parameters\nnb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", featuresCol=\"features\")\n\n# train the model\nmodel = nb.fit(training_m1)\n\n# select example rows to display.\npredictions = model.transform(valid_m1)\npredictions.show()\n\n# compute accuracy on the test set\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n                                              metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test set accuracy = \" + str(accuracy))"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["## We are implementing Naive Bayes for Model2\n\nfrom pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# create the trainer and set its parameters\nnb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", featuresCol=\"features\")\n\n# train the model\nmodel = nb.fit(training_m2)\n\n# select example rows to display.\npredictions = model.transform(valid_m2)\npredictions.show()\n\n# compute accuracy on the test set\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n                                              metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test set accuracy = \" + str(accuracy))"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["## We are implementing Naive Bayes for Model3\n\nfrom pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# create the trainer and set its parameters\nnb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", featuresCol=\"features\")\n\n# train the model\nmodel = nb.fit(training_m3)\n\n# select example rows to display.\npredictions = model.transform(valid_m3)\npredictions.show()\n\n# compute accuracy on the test set\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n                                              metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test set accuracy = \" + str(accuracy))"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["## Implementing Neural Networks for Model 1##\n\nfrom pyspark.ml.classification import MultilayerPerceptronClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nlayers = [3, 2, 2, 2]\n\n# create the trainer and set its parameters\ntrainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234, featuresCol=\"features\")\n\n# train the model\nmodel = trainer.fit(training_m1)\n\n# compute accuracy on the test set\nresult = model.transform(valid_m1)\npredictionAndLabels = result.select(\"prediction\", \"label\")\nevaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\nprint(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["## Implementing Neural Networks for Model 2##\n\nfrom pyspark.ml.classification import MultilayerPerceptronClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nlayers = [7, 4, 3, 2]\n\n# create the trainer and set its parameters\ntrainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234, featuresCol=\"features\")\n\n# train the model\nmodel = trainer.fit(training_m2)\n\n# compute accuracy on the test set\nresult = model.transform(valid_m2)\npredictionAndLabels = result.select(\"prediction\", \"label\")\nevaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\nprint(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["## Implementing Neural Networks for Model 3##\n\nfrom pyspark.ml.classification import MultilayerPerceptronClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nlayers = [4, 3, 2, 2]\n\n# create the trainer and set its parameters\ntrainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234, featuresCol=\"features\")\n\n# train the model\nmodel = trainer.fit(training_m3)\n\n# compute accuracy on the test set\nresult = model.transform(valid_m3)\npredictionAndLabels = result.select(\"prediction\", \"label\")\nevaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\nprint(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["## Implementing Neural Network for Model1\n## Implementing Logistic Regression in hidden Layers as a classifier algorithm\n\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.linalg import Vectors\nlr = LogisticRegression()\ngrid = ParamGridBuilder().addGrid(lr.maxIter, [0, 1]).build()\nevaluator = BinaryClassificationEvaluator()\ncv = CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator, numFolds=10)\ncvModel = cv.fit(training_m1)\ncvModel.avgMetrics[0]\n\nevaluator.evaluate(cvModel.transform(valid_m1))\n"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["\n## Implementing Neural Network for Model2\n## Implementing Logistic Regression in hidden Layers as a classifier algorithm\n\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.linalg import Vectors\nlr = LogisticRegression()\ngrid = ParamGridBuilder().addGrid(lr.maxIter, [0, 1]).build()\nevaluator = BinaryClassificationEvaluator()\ncv = CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator, numFolds=10)\ncvModel = cv.fit(training_m2)\ncvModel.avgMetrics[0]\n\nevaluator.evaluate(cvModel.transform(valid_m2))\n"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["## Implementing Neural Network for Model3\n## Implementing Logistic Regression in hidden Layers as a classifier algorithm\n\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.linalg import Vectors\nlr = LogisticRegression()\ngrid = ParamGridBuilder().addGrid(lr.maxIter, [0, 1]).build()\nevaluator = BinaryClassificationEvaluator()\ncv = CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator, numFolds=10)\ncvModel = cv.fit(training_m3)\ncvModel.avgMetrics[0]\n\nevaluator.evaluate(cvModel.transform(valid_m3))"],"metadata":{},"outputs":[],"execution_count":48}],"metadata":{"name":"Project_final_Code","notebookId":1368684205808337},"nbformat":4,"nbformat_minor":0}
